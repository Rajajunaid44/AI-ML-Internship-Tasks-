{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2586feec-31d7-4555-8837-3e08f0f3d779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,pipeline\n",
    "print(\"Loading the model.\")\n",
    "model=\"microsoft/DialoGPT-small\"\n",
    "tokenize=AutoTokenizer.from_pretrained(model)\n",
    "model2=AutoModelForCausalLM.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6aec614-d04d-4aac-9c06-17aabca75dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "#pipelining\n",
    "pipe=pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model2,\n",
    "    tokenizer=tokenize,\n",
    "    max_length=200,\n",
    "    temperature=0.6,\n",
    "    do_sample=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5659d757-b156-4df8-89dc-4fc8e5171e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using hugging face\n",
    "llm=HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75f3c046-9bb0-401f-8331-15b27e3f84a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=[]\n",
    "def get_context_prompt(user_input,history):\n",
    "    \"\"\"Creating a prompt\"\"\"\n",
    "    if not history:\n",
    "        return user_input\n",
    "    context=\"\"\n",
    "    recent=history[-6:] if len(history) > 6 else history\n",
    "    for msg in recent_history:\n",
    "        if isinstance(msg,HumanMessage):\n",
    "            context+=f\"Human: {msg.content}\\n\"\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            context+=f\"Assistant: {msg.content}\\n\"\n",
    "    return f\"{context}Human: {user_input}\\nAssistant:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a50866ce-b518-4b96-a101-087c5f583526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_response(response):\n",
    "    \"\"\"Cleaning the model response\"\"\"\n",
    "    lines=response.split('\\n')\n",
    "    for line in lines:\n",
    "        if line.startswith('Assistant:') and len(line) > 11:\n",
    "            return line[11:].strip()\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a5dd44-3622-4301-9972-7931fff6f5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: name 'recent_history' is not defined\n",
      "Bot: Sorry this cannot be process check and try again.\n"
     ]
    }
   ],
   "source": [
    "# Main chat loop\n",
    "while True:\n",
    "    user_input = input(\"\\nYou: \").strip()\n",
    "    if user_input.lower()=='quit':\n",
    "        print(\"exiting good bye\")\n",
    "        break\n",
    "    elif user_input.lower()=='clear':\n",
    "        chat_history=[]\n",
    "        print(\"Clearing the history\")\n",
    "        continue\n",
    "    elif not user_input:\n",
    "        continue  \n",
    "    try:\n",
    "        prompt=get_context_prompt(user_input, chat_history)\n",
    "        response=llm.invoke(prompt)\n",
    "        response=clean_response(response)\n",
    "        chat_history.append(HumanMessage(content=user_input))\n",
    "        chat_history.append(AIMessage(content=response))\n",
    "        print(f\"Bot: {response}\")     \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Bot: Sorry this cannot be process check and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f36a8c-d02a-4a93-ae3f-cf8d3b5e8942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
