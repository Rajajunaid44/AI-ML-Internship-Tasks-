{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2586feec-31d7-4555-8837-3e08f0f3d779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,pipeline\n",
    "print(\"Loading the model.\")\n",
    "model=\"microsoft/DialoGPT-small\"\n",
    "tokenize=AutoTokenizer.from_pretrained(model)\n",
    "model2=AutoModelForCausalLM.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e6aec614-d04d-4aac-9c06-17aabca75dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "#pipelining\n",
    "pipe=pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model2,\n",
    "    tokenizer=tokenize,\n",
    "    max_length=200,\n",
    "    temperature=0.6,\n",
    "    do_sample=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5659d757-b156-4df8-89dc-4fc8e5171e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using hugging face\n",
    "llm=HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "82a5dd44-3622-4301-9972-7931fff6f5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(user_input, history):\n",
    "    if not history:\n",
    "        return user_input\n",
    "    context=\"\"\n",
    "    recent_history=history[-6:] if len(history)>6 else history\n",
    "    for msg in recent_history:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            context+=f\"Human: {msg.content}\\n\"\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            context+=f\"Assistant: {msg.content}\\n\"\n",
    "    return f\"{context}Human: {user_input}\\nAssistant:\"\n",
    "def clean(response):\n",
    "    lines=response.split('\\n')\n",
    "    for line in lines:\n",
    "        if line.startswith('Assistant:') and len(line) > 11:\n",
    "            return line[11:].strip()\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "79f36a8c-d02a-4a93-ae3f-cf8d3b5e8942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot ready to go type 'quit' to exit or 'clear' to clear history.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: hello??\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot ready to go type 'quit' to exit or 'clear' to clear history.\")\n",
    "while True:\n",
    "    user_input=input(\"\\nYou: \").strip()\n",
    "    if user_input.lower()=='quit':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    elif user_input.lower()=='clear':\n",
    "        chat_history=[]\n",
    "        print(\"Cleared history\")\n",
    "        continue\n",
    "    elif not user_input:\n",
    "        continue\n",
    "    try:\n",
    "        prompt=prompt(user_input, chat_history)\n",
    "        response=llm.invoke(prompt)\n",
    "        bot_response=clean(response)\n",
    "        chat_history.append(HumanMessage(content=user_input))\n",
    "        chat_history.append(AIMessage(content=bot_response))\n",
    "        print(f\"Bot: {bot_response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error:{e}\")\n",
    "        print(\"Bot:Sorry this cannot be process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de79782-5578-4333-ba52-25b1808efc16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
